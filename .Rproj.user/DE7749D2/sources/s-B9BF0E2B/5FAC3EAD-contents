---
title: "Introduction to Stan"
author: "Monica Alexander"
date: "February 12 2020"
output: 
  pdf_document:
    number_sections: true
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Introduction

Today we will be starting off using Stan, looking at the kid's test score data set (available in resources for the [Gelman Hill textbook](https://mc-stan.org/rstanarm/reference/rstanarm-datasets.html)). 

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
```


The data look like this:

```{r}
kidiq <- read_rds("../data/kidiq.RDS") 
kidiq
```
As well as the kid's test scores, we have a binary variable indicating whether or not the mother completed high school, the mother's IQ and age. 

\pagebreak
# Descriptives

## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type

```{r}
kidiq %>% 
  ggplot(aes(x=kid_score, y=mom_iq)) + geom_point() + 
  geom_smooth(method='lm', formula= y~x)
```
There is a weak correlation between kid and mom IQ score.

```{r}
kidiq %>% 
  mutate(kid_smarter_than_mom = case_when(
      kid_score > mom_iq ~ 1,
      TRUE ~ 0
    )) %>% 
  summarise(count_kid_smarter = sum(kid_smarter_than_mom) / n())
```
Not a chart, but an interesting summary statistics. 25% of kids have a higher IQ than their mother. 

```{r}
ggplot(data=kidiq, mapping=aes(x=mom_iq, fill=as.character(mom_hs))) + 
  geom_density(alpha=0.3) + 
  labs(x='Mom IQ', y='Density', fill='Mother Attended High School')
```
Mothers who attended high school are more likely to have higher IQ. 

# Estimating mean, no covariates

In class we were trying to estimate the mean and standard deviation of the kid's test scores. The `kids2.stan` file contains a Stan model to do this. If you look at it, you will notice the first `data` chunk lists some inputs that we have to define: the outcome variable `y`, number of observations `N`, and the mean and standard deviation of the prior on `mu`. Let's define all these values in a `data` list.


```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 100

data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
```


Now we can run the model:

```{r include=FALSE}
fit <- stan(file = "../code/models/kids2.stan",
            data = data)
```

Look at the summary

```{r}
fit
```

Traceplot

```{r}
traceplot(fit)
```

All looks fine. 
\pagebreak
## Understanding output

What does the model actually give us? A number of samples from the posteriors. To see this, we can use `extract` to get the samples. 

```{r}
post_samples <- extract(fit)
```

This is a list, and in this case, each element of the list has 4000 samples. E.g. quickly plot a histogram of mu

```{r}
hist(post_samples[["mu"]])
```


\pagebreak
## Plot estimates

There are a bunch of packages, built-in functions that let you plot the estimates from the model, and I encourage you to explore these options (particularly in `bayesplot`, which we will most likely be using later on). I like using the `tidybayes` package, which allows us to easily get the posterior samples in a tidy format (e.g. using gather draws to get in long format). Once we have that, it's easy to just pipe and do ggplots as usual. `tidybayes` also has a bunch of fun visualizations, see more info here: https://mjskay.github.io/tidybayes/articles/tidybayes.html#introduction


Get the posterior samples for mu and sigma in long format:

```{r}
dsamples <- fit %>%
  gather_draws(mu, sigma) 
dsamples
```
\pagebreak
Let's plot the density of the posterior samples for mu and add in the prior distribution

```{r}
dsamples %>% 
  filter(.variable == "mu") %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")
```
\pagebreak
## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 

```{r}
y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 0.1

data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)

fit2 <- stan(file = "../code/models/kids2.stan",
            data = data)
```

The last fit:

```{r}
fit
```

The new fit:
```{r}
fit2
```

The estimates are not the same. The new highly confident model is harder to move with new data. 

\pagebreak
```{r}
dsamples <- fit2 %>%
  gather_draws(mu, sigma) 

dsamples %>% 
  filter(.variable == "mu") %>% 
  ggplot(aes(.value, color = "posterior")) + geom_density(size = 1) + 
  xlim(c(70, 100)) + 
  stat_function(fun = dnorm, 
        args = list(mean = mu0, 
                    sd = sigma0), 
        aes(colour = 'prior'), size = 1) +
  scale_color_manual(name = "", values = c("prior" = "red", "posterior" = "black")) + 
  ggtitle("Prior and posterior for mean test scores") + 
  xlab("score")

```
\pagebreak
# Adding covariates

Now let's see how kid's test scores are related to mother's education. We want to run the simple linear regression

$$
Score = \alpha + \beta X
$$
where $X = 1$ if the mother finished high school and zero otherwise. 

`kid3.stan` has the stan model to do this. Notice now we have some inputs related to the design matrix $X$ and the number of covariates (in this case, it's just 1).

Let's get the data we need and run the model. 

```{r}
X <- as.matrix(kidiq$mom_hs, ncol = 1)
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = "../code/models/kids3.stan",
            data = data, 
            iter = 1000)
```
\pagebreak
## Question 3

Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 

```{r}
model <- lm(data=kidiq, formula=kid_score ~ mom_hs)
model
```

```{r}
fit2
```

`alpha` and `beta[1]` are very close to the coefficients we got from `lm()`

\pagebreak
## Plotting results

It might be nice to plot the posterior samples of the estimates for the non-high-school and high-school mothered kids. Here's some code that does this: notice the `beta[condition]` syntax. Also notice I'm using `spread_draws`, because it's easier to calculate the estimated effects in wide format

```{r}
fit2 %>%
  spread_draws(alpha, beta[condition], sigma) %>% 
     mutate(nhs = alpha, # no high school is just the intercept
          hs = alpha + beta) %>% 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") %>% 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeyeh() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
```

\pagebreak
## Question 4

Add in mother's IQ as a covariate and rerun the model. You will probably want to mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

```{r}
X <- as.matrix(kidiq %>% select(mom_hs, mom_iq),  ncol = 2)
X
K <- 2

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = "../code/models/kids4.stan",
            data = data, 
            iter = 1000)
```
```{r}
summary(fit2)[['summary']][1:3,1]
```

A mother compelting high school adds 5 iq points to her son's iq and for each unit of iq increase of mother a son's iq increases by about 1/2. 
\pagebreak
## Question 5 

Confirm the results from Stan agree with `lm()`

```{r}
model2 = lm(data=kidiq, formula=kid_score ~ mom_hs + mom_iq)
```

```{r}
model2
```
```{r}
summary(fit2)[['summary']][1:3,1]
```

They match closely!
\pagebreak
## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 

```{r}
fit2 %>% 
  spread_draws(alpha, beta[condition]) %>% 
  pivot_wider(names_from='condition', values_from='beta', names_prefix='beta') %>% 
  mutate(hs=alpha+beta1 + beta2 * 110, nhs=alpha+beta2 * 110) %>% 
  pivot_longer(nhs:hs, names_to = "education", values_to = "estimated_score") %>% 
  ggplot(aes(y = education, x = estimated_score)) +
  stat_halfeyeh() + 
  theme_bw() + 
  ggtitle("Posterior estimates of scores by education level of mother")
```


